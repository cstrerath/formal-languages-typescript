{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d894ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { display } from \"tslab\";\n",
    "import { readFileSync } from \"fs\";\n",
    "\n",
    "const css: string = readFileSync(\"../style.css\", \"utf8\");\n",
    "display.html(`<style>${css}</style>`);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a68011",
   "metadata": {},
   "source": [
    "The following example has been adapted from the official Ply documentation and ported to TypeScript using Lezer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49c3ba",
   "metadata": {},
   "source": [
    "## A Tokenizer for Numbers and the Arithmetical Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc3416",
   "metadata": {},
   "source": [
    "In this notebook, we use a **declarative** approach using **Lezer**. Lezer is a modern, incremental parser generator often used in web editors (like CodeMirror).\n",
    "\n",
    "**Lezer** requires us to write a **Grammar**.\n",
    "\n",
    "Usually, a grammar defines both the tokens (words) and the sentence structure (syntax). However, since we are currently focusing only on **Tokenization** (Scanning) and have not yet covered Grammars or LR-Parsers, we will configure Lezer to act as a pure Scanner.\n",
    "\n",
    "### 1. Defining the Lexical Rules\n",
    "\n",
    "We define our tokens using a grammar string. We define rules using a notation similar to **Regular Expressions**.\n",
    "\n",
    "We define a \"permissive\" top-level rule:\n",
    "$$\\text{Script} \\rightarrow (\\text{AnyToken})^*$$\n",
    "\n",
    "This tells the parser: \"A script is simply a sequence of any known tokens, in any order.\" This disables syntax checking for now, allowing us to focus entirely on how raw text is split into tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338af48e",
   "metadata": {},
   "source": [
    "## Step-by-Step Lexer Definition\n",
    "\n",
    "Instead of writing one giant grammar string, we will build our Lexer definition piece by piece. This allows us to inspect exactly how tokens are defined and structured.\n",
    "\n",
    "First, we need the generator tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e06693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { buildParser } from \"@lezer/generator\";\n",
    "import { Tree, TreeCursor } from \"@lezer/common\";\n",
    "import { LRParser } from \"@lezer/lr\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a95c87a",
   "metadata": {},
   "source": [
    "### 1. The Entry Point\n",
    "\n",
    "Since we are building a **Scanner** (not a Parser yet), we want to be very permissive. We define a rule named `Script` that accepts a sequence of *any* valid tokens (`token*`).\n",
    "\n",
    "This creates a \"flat\" structure. It tells Lezer: *\"Don't check if the order makes sense mathematically, just check if the words exist.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dadc52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "const entryPoint: string = `\n",
    "  @top Script { token* }\n",
    "`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd82e984",
   "metadata": {},
   "source": [
    "### 2. The Token Union\n",
    "\n",
    "We need a rule that lists all possible things that can appear in our token stream. This acts as a central registry for our scanner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "const tokenStructure: string = `\n",
    "  token {\n",
    "    Number |\n",
    "    Identifier |\n",
    "    Plus | Minus | Times | Divide |\n",
    "    LParen | RParen\n",
    "  }\n",
    "`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3198af",
   "metadata": {},
   "source": [
    "### 3. Defining Tokens in Lezer\n",
    "\n",
    "In **Lezer**, we define all lexical rules inside a `@tokens { ... }` block within the grammar string. This block acts as the dictionary for our language, mapping **Token Names** to **Patterns**.\n",
    "\n",
    "#### 3.1 Literal Tokens (Operators)\n",
    "\n",
    "The simplest tokens are fixed strings, such as mathematical operators or parentheses. We define them by mapping a name (Capitalized) to a string literal.\n",
    "\n",
    "* **Syntax:** `TokenName { \"string\" }`\n",
    "* **Note:** Unlike standard Regular Expressions, we do not need to escape special characters like `+` or `*` here. Lezer treats quoted strings as exact text matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd90f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "const simpleTokens: string = `\n",
    "  @tokens {\n",
    "    // 1. Literal Matches (Operators & Punctuation)\n",
    "    Plus   { \"+\" }\n",
    "    Minus  { \"-\" }\n",
    "    Times  { \"*\" }\n",
    "    Divide { \"/\" }\n",
    "    LParen { \"(\" }\n",
    "    RParen { \")\" }\n",
    "`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e4596",
   "metadata": {},
   "source": [
    "#### 3.2 Complex Patterns (Regular Expressions)\n",
    "\n",
    "For dynamic values like Numbers or Identifiers, we cannot list every possibility. Instead, we use **Patterns**.\n",
    "\n",
    "Lezer uses a notation very similar to Regular Expressions (Regex), but with a key difference regarding **Character Sets**:\n",
    "\n",
    "1.  **Character Classes `$[...]`**:\n",
    "    In standard Regex, `[0-9]` matches any digit. In Lezer, you **must** prefix the brackets with a dollar sign `$`.\n",
    "    * Standard Regex: `[a-z]` $\\rightarrow$ Lezer: `$[a-z]`\n",
    "    \n",
    "2.  **Quantifiers**:\n",
    "    These work exactly like standard Regex:\n",
    "    * `*` : Match zero or more times.\n",
    "    * `+` : Match one or more times.\n",
    "    * `|` : Logical OR (Alternative).\n",
    "\n",
    "**The Number Definition:**\n",
    "We define a number using this pattern:\n",
    "`'0' | $[1-9] $[0-9]*`\n",
    "\n",
    "This logic is precise:\n",
    "* **Either** match the character `'0'` exactly.\n",
    "* **OR** match a non-zero digit (`1-9`) followed by any number of digits (`0-9`).\n",
    "* *Why?* This prevents a sequence like `007` from being interpreted as a single number \"Seven\". Instead, our scanner will see it as three separate tokens: `0`, `0`, and `7`.\n",
    "\n",
    "**The Identifier Definition:**\n",
    "`$[a-zA-Z_] $[a-zA-Z0-9_]*`\n",
    "* Must start with a letter or underscore.\n",
    "* Can be followed by letters, numbers, or underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ffc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "const complexTokens: string = `\n",
    "    // 2. Complex Patterns using Lezer Regex Syntax\n",
    "\n",
    "    // Number: Matches '0' OR (1-9 followed by digits)\n",
    "    Number { '0' | $[1-9] $[0-9]* }\n",
    "\n",
    "    // Identifier: Starts with Letter/Underscore, then alphanumeric\n",
    "    Identifier { $[a-zA-Z_] $[a-zA-Z0-9_]* }\n",
    "`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa02f4f",
   "metadata": {},
   "source": [
    "#### 3.3 Handling Whitespace\n",
    "\n",
    "We need to explicitly define what \"whitespace\" is so we can tell the parser to ignore it later. We define a token named `space` that matches spaces, tabs (`\\t`), newlines (`\\n`), and carriage returns (`\\r`).\n",
    "\n",
    "* **Pattern:** `$[ \\t\\n\\r]+` (One or more whitespace characters).\n",
    "\n",
    "**Important Note on Escaping:**\n",
    "Lezer is flexible regarding escape sequences. It accepts both:\n",
    "1.  **Single Backslash (`\\n`):** Creates an actual newline character inside the TypeScript string. Lezer interprets this correctly as whitespace.\n",
    "2.  **Double Backslash (`\\\\n`):** Creates the literal text characters `\\` followed by `n`. Lezer parses this text sequence as a \"newline rule\".\n",
    "\n",
    "**Why do we use double backslashes below?**\n",
    "We use `\\\\t` and `\\\\n` purely for **visualization purposes**. When we print the `finalGrammar` string to the console later, we want to read the text `\\n` (the definition) rather than seeing an invisible line break.\n",
    "\n",
    "Finally, we close the `@tokens` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "const whitespaceAndClose: string = `\n",
    "    // 3. Whitespace Definition\n",
    "    space { $[ \\\\t\\\\n\\\\r]+ }\n",
    "  }\n",
    "`;\n",
    "\n",
    "const tokenDefinitions: string = simpleTokens + complexTokens + whitespaceAndClose;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec12e2",
   "metadata": {},
   "source": [
    "#### Note on Values\n",
    "\n",
    "Unlike some other lexing tools, Lezer does not automatically convert the text `\"42\"` into the integer `42` during this step.\n",
    "\n",
    "Lezer's job is to produce a **Syntax Tree** containing the raw text segments. Converting these strings into actual numbers (e.g., using `parseInt`) happens in a later step when we process the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a0d7da",
   "metadata": {},
   "source": [
    "### 4. Skip Logic\n",
    "\n",
    "Finally, we tell Lezer which tokens should be ignored in the output. Usually, we want to discard spaces, tabs, and line breaks to keep the result clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85210f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "const skipStrategy: string = `\n",
    "  @skip { space }\n",
    "`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0db87",
   "metadata": {},
   "source": [
    "### 5. Assembly and Execution\n",
    "\n",
    "Now we concatenate all string parts to form the full grammar definition and generate the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "const finalGrammar: string = entryPoint + tokenStructure + tokenDefinitions + skipStrategy;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15565c",
   "metadata": {},
   "source": [
    "### Inspecting the Complete Grammar\n",
    "\n",
    "Before we compile the parser, let's verify the final grammar string we have assembled.\n",
    "\n",
    "**Note on Best Practices:**\n",
    "In this notebook, we split the grammar into multiple string variables (`entryPoint`, `tokenStructure`, etc.) solely for **educational purposes** to explain each section individually.\n",
    "\n",
    "In a real-world project, you would typically write the entire grammar in:\n",
    "1.  A single **Template Literal** (one large backtick string).\n",
    "2.  Or, more commonly, in a separate file (e.g., `arithmetic.grammar`) which is then compiled by the Lezer CLI tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalGrammar;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c578bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "const parser : LRParser = buildParser(finalGrammar);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54cc70c",
   "metadata": {},
   "source": [
    "Let's test the generated lexer with the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "const data : string = `\n",
    "       3 + 4 * 10 + 007 + (-20) * 2\n",
    "       42/À\n",
    "       a\n",
    "       `;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083bf50f",
   "metadata": {},
   "source": [
    "Here is the input string we will tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db12dd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e0cd0",
   "metadata": {},
   "source": [
    "### 6. Generating the Syntax Tree\n",
    "\n",
    "To verify our Lexer logic, we first need to instantiate the parser and process our input data.\n",
    "\n",
    "**Function Specification:**\n",
    "We utilize the `parse` method of the generated parser.\n",
    "* **Input:** Source string $S \\in \\Sigma^*$ (where $\\Sigma$ is the set of Unicode characters).\n",
    "* **Output:** A Concrete Syntax Tree (CST) $T$.\n",
    "\n",
    "After parsing, we initialize a `TreeCursor` $C$. This cursor acts as an iterator that traverses $T$ in a **depth-first, pre-order** manner. This allows us to visit every token exactly in the order they appear in the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e93c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "const tree: Tree = parser.parse(data);\n",
    "const cursor: TreeCursor = tree.cursor();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c565e99",
   "metadata": {},
   "source": [
    "### 7. Transforming the Tree into a Token List\n",
    "\n",
    "To separate the scanning logic from the presentation logic, we define a transformation function `extractTokens`. This function flattens the Lezer CST into a linear sequence of typed objects.\n",
    "\n",
    "#### 7.1 Data Structure Definition\n",
    "\n",
    "We define a `Token` as a 5-tuple:\n",
    "$$ T = ( \\text{type}, \\text{value}, \\text{start}, \\text{end}, \\text{isError} ) $$\n",
    "\n",
    "Where:\n",
    "* $\\text{type} \\in \\text{String}$: The name of the token class (e.g., \"Number\", \"Plus\").\n",
    "* $\\text{value} \\in \\text{String}$: The literal substring from the source code.\n",
    "* $\\text{start}, \\text{end} \\in \\mathbb{N}_0$: The indices in the source string.\n",
    "* $\\text{isError} \\in \\text{Boolean}$: A flag indicating lexical failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bff5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "interface Token {\n",
    "  type: string;\n",
    "  value: string;\n",
    "  start: number;\n",
    "  end: number;\n",
    "  isError: boolean;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06991856",
   "metadata": {},
   "source": [
    "#### 7.2 The Extraction Algorithm\n",
    "\n",
    "**Input:** A Syntax Tree $Tree$ and the Source String $S$.\n",
    "**Output:** A sequence of Tokens $L = [t_1, t_2, \\dots, t_n]$.\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. Initialize an empty list $L \\leftarrow []$.\n",
    "2. Initialize cursor $C$ at the root of $Tree$.\n",
    "3. **Do** while $C$ can move to the next node:\n",
    "    * Let $N$ be the name of the current node.\n",
    "    * **If** $N \\in \\{ \\text{\"Script\"}, \\text{\"token\"} \\}$ (Grammar Wrappers), **Continue**.\n",
    "    * Extract substring $V = S[C.\\text{from} \\dots C.\\text{to}]$.\n",
    "    * Determine type: if $N = \\text{\"⚠\"}$, then $type \\leftarrow \\text{\"Error\"}$, else $type \\leftarrow N$.\n",
    "    * Construct token $t = (type, V, C.\\text{from}, C.\\text{to}, N = \\text{\"⚠\"})$.\n",
    "    * Append $t$ to $L$.\n",
    "4. **Return** $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31598a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function extractTokens(tree: Tree, source: string): Token[] {\n",
    "  const cursor: TreeCursor = tree.cursor();\n",
    "  const tokens: Token[] = [];\n",
    "\n",
    "  do {\n",
    "    const typeName = cursor.name;\n",
    "    \n",
    "    if (typeName === \"Script\" || typeName === \"token\") {\n",
    "      continue;\n",
    "    }\n",
    "\n",
    "    const value = source.substring(cursor.from, cursor.to);\n",
    "    const isError = typeName === \"⚠\";\n",
    "\n",
    "    const token: Token = {\n",
    "      type: isError ? \"Error\" : typeName,\n",
    "      value: value,\n",
    "      start: cursor.from,\n",
    "      end: cursor.to,\n",
    "      isError: isError\n",
    "    };\n",
    "\n",
    "    tokens.push(token);\n",
    "\n",
    "  } while (cursor.next());\n",
    "\n",
    "  return tokens;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53a3a4",
   "metadata": {},
   "source": [
    "### 8. Result Inspection\n",
    "\n",
    "Finally, we apply our transformation function to the parsed tree and iterate over the resulting typed array to display the token stream.\n",
    "\n",
    "**Process:**\n",
    "1.  **Input:** The raw `tree` and `data` string.\n",
    "2.  **Transformation:** Call `extractTokens(tree, data)` to obtain `Token[]`.\n",
    "3.  **Output:** Print each token to the console, formatting newlines (`\\n`) for visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38bacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "const tokenList: Token[] = extractTokens(tree, data);\n",
    "\n",
    "console.log(\"Token Stream Analysis (Typed):\");\n",
    "console.log(\"------------------------------\");\n",
    "\n",
    "tokenList.forEach((t: Token) => {\n",
    "  const displayVal = t.value.replace(/\\n/g, \"\\\\n\");\n",
    "  \n",
    "  if (t.isError) {\n",
    "     console.log(`Illegal character: '${displayVal}' at pos ${t.start}`);\n",
    "  } else {\n",
    "     console.log(`Token(${t.type.padEnd(11)}, '${displayVal}', Pos: ${t.start})`);\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0724fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
