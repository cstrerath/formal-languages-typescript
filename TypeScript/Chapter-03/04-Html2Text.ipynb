{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { display } from \"tslab\";\n",
    "import { readFileSync } from \"fs\";\n",
    "\n",
    "const css : string = readFileSync(\"../style.css\", \"utf8\");\n",
    "display.html(`<style>${css}</style>`);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e48bae9",
   "metadata": {},
   "source": [
    "# Converting HTML to Text\n",
    "\n",
    "This notebook demonstrates how to build a simple but effective HTML-to-text converter using **TypeScript** and the parsing library [`lezer`](https://lezer.codemirror.net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f907e46",
   "metadata": {},
   "source": [
    "## The Goal\n",
    "\n",
    "Our objective is to extract the readable, plain text content from a given HTML document. We will use the HTML source code from the homepage of [Prof. Dr. Karl Stroetmann](http://wwwlehre.dhbw-stuttgart.de/~stroetma/) as our example data. \n",
    "\n",
    "To achieve this, we define a grammar in TypeScript that distinguishes  \n",
    "between different HTML sections such as `<head>`, `<script>`, and normal text.\n",
    "\n",
    "First, let's load our example HTML data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e774804",
   "metadata": {},
   "outputs": [],
   "source": [
    "const data : string = `\n",
    "<html>\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>Homepage of Prof. Dr. Karl Stroetmann</title>\n",
    "    <link type=\"text/css\" rel=\"stylesheet\" href=\"style.css\" />\n",
    "    <link href=\"http://fonts.googleapis.com/css?family=Rochester&subset=latin,latin-ext\"\n",
    "          rel=\"stylesheet\" type=\"text/css\">\n",
    "    <link href=\"http://fonts.googleapis.com/css?family=Pacifico&subset=latin,latin-ext\"\n",
    "          rel=\"stylesheet\" type=\"text/css\">\n",
    "    <link href=\"http://fonts.googleapis.com/css?family=Cabin+Sketch&subset=latin,latin-ext\" rel=\"stylesheet\" type=\"text/css\">\n",
    "    <link href=\"http://fonts.googleapis.com/css?family=Sacramento\" rel=\"stylesheet\" type=\"text/css\">\n",
    "  </head>\n",
    "  <body>\n",
    "    <hr/>\n",
    "\n",
    "    <div id=\"table\">\n",
    "      <header>\n",
    "        <h1 id=\"name\">Prof. Dr. Karl Stroetmann</h1>\n",
    "      </header>\n",
    "\n",
    "      <div id=\"row1\">\n",
    "        <div class=\"right\">\n",
    "          <a id=\"dhbw\" href=\"http://www.ba-stuttgart.de\">Duale Hochschule Baden-W&uuml;rttemberg</a>\n",
    "          <br/>Coblitzallee 1-9\n",
    "          <br/>68163 Mannheim\n",
    "          <br/>Germany\n",
    "\t  <br>\n",
    "          <br/>Office: &nbsp;&nbsp;&nbsp; Raum 344B\n",
    "          <br/>Phone:&nbsp;&nbsp;&nbsp; +49 621 4105-1376\n",
    "          <br/>Fax:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; +49 621 4105-1194\n",
    "          <br/>Skype: &nbsp;&nbsp;&nbsp; karlstroetmann\n",
    "        </div>  \n",
    "\n",
    "\n",
    "        <div id=\"links\">\n",
    "          <strong class=\"some\">Some links:</strong>\n",
    "          <ul class=\"inlink\">\n",
    "            <li class=\"inlink\">\n",
    "\t      My <a class=\"inlink\" href=\"https://github.com/karlstroetmann?tab=repositories\">lecture notes</a>,\n",
    "              as well as the programs presented in class, can be found\n",
    "              at <br>\n",
    "              <a class=\"inlink\" href=\"https://github.com/karlstroetmann?tab=repositories\">https://github.com/karlstroetmann</a>.\n",
    "              \n",
    "            </li>\n",
    "            <li class=\"inlink\">Most of my papers can be found at <a class=\"inlink\" href=\"https://www.researchgate.net/\">researchgate.net</a>.</li>\n",
    "            <li class=\"inlink\">The programming language SetlX can be downloaded at <br>\n",
    "              <a href=\"http://randoom.org/Software/SetlX\"><tt class=\"inlink\">http://randoom.org/Software/SetlX</tt></a>.\n",
    "            </li>\n",
    "          </ul>\n",
    "        </div>\n",
    "      </div>\n",
    "    </div>\n",
    "    \n",
    "    <div id=\"intro\">\n",
    "      As I am getting old and wise, I have to accept the limits of\n",
    "      my own capabilities.  I have condensed these deep philosophical\n",
    "      insights into a most beautiful pearl of poetry.  I would like \n",
    "      to share these humble words of wisdom:\n",
    "      \n",
    "      <div class=\"poetry\">\n",
    "        I am a teacher by profession,    <br>\n",
    "        mostly really by obsession;      <br>\n",
    "        But even though I boldly try,    <br>\n",
    "        I just cannot teach <a href=\"flying-pig.jpg\" id=\"fp\">pigs</a> to fly.</br>\n",
    "        Instead, I slaughter them and fry.\n",
    "      </div>\n",
    "      \n",
    "      <div class=\"citation\">\n",
    "        <div class=\"quote\">\n",
    "          Any sufficiently advanced poetry is indistinguishable from divine wisdom.\n",
    "        </div>\n",
    "        <div id=\"sign\">His holiness Pope Hugo &#8555;.</div>\n",
    "      </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display.html(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba750b4",
   "metadata": {},
   "source": [
    "The original web page is still available at https://wwwlehre.dhbw-stuttgart.de/~stroetma/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f105f6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b61710",
   "metadata": {},
   "source": [
    "Before we can build our HTML lexer, we need to install and import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9430c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { buildParser } from '@lezer/generator';\n",
    "import { Tree, TreeCursor } from '@lezer/common';\n",
    "import { LRParser } from '@lezer/lr';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb2c3e",
   "metadata": {},
   "source": [
    "## Defining the Grammar\n",
    "In Lezer, we define tokens and the document structure in a declarative grammar string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef099264",
   "metadata": {},
   "source": [
    "## Token Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c220b",
   "metadata": {},
   "source": [
    "We need to define tokens for:\n",
    "\n",
    "```\n",
    "- **Tags**: HTML tags like `<br/>`, `<div>`, `</div>` which should generally be ignored.\n",
    "```\n",
    "\n",
    "- **Named Entities**: Special characters like `&amp;` or `&uuml;`.\n",
    "- **Unicode Entities**: Numeric references like `&#8594;`.\n",
    "- **Content**: Regular text.\n",
    "- **Linebreaks**: To maintain paragraph formatting.\n",
    "\n",
    "```\n",
    "Special handling is required for `<head>` and `<script>` blocks, as their content should typically be excluded from the plain text output.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa605d1d",
   "metadata": {},
   "source": [
    "### 1. Document Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687195d7",
   "metadata": {},
   "source": [
    "First, we define the entry point (`@top`). A document consists of a sequence of various elements: blocks to ignore (Head, Script), structural elements (Tags, Linebreaks), and actual content (Text, Entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ce835",
   "metadata": {},
   "outputs": [],
   "source": [
    "const entryPoint = `\n",
    "  @top Document { (HeadBlock | ScriptBlock | Tag | Entity | Unicode | Linebreak | Content | any)* }\n",
    "`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238769a3",
   "metadata": {},
   "source": [
    "### 2. Basic Token Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ecb5c9",
   "metadata": {},
   "source": [
    "We start the `@tokens` block. Here we define the basic building blocks of HTML: tags and line breaks.\n",
    "\n",
    "* **`Tag`**: Matches standard HTML tags starting with `<` and ending with `>`.\n",
    "* **`Linebreak`**: Captures newlines to preserve formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "const basicTokens = `\n",
    "  @tokens {\n",
    "    Tag { \"<\" ![:]* \">\" }\n",
    "    Linebreak { $[\\\\n\\\\r]+ }\n",
    "`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58586fe2",
   "metadata": {},
   "source": [
    "### 3. Entity Definitions\n",
    "Next, we define special character entities. We distinguish between named entities (like `&amp;`) and numeric unicode entities (like `&#8594;`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "const entityTokens = `\n",
    "    Entity { \"&\" $[a-zA-Z]+ \";\" }\n",
    "    Unicode { \"&#\" $[0-9]+ \";\" }\n",
    "`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a1d325",
   "metadata": {},
   "source": [
    "### 4. Special Block Definitions\n",
    "```\n",
    "This is the most complex part. We need to define blocks for `<head>` and `<script>` tags so that we can ignore their *entire* content (including what looks like text inside them).\n",
    "```\n",
    "\n",
    "* **`ScriptBlock`**: Matches the opening `<script...>`, any content inside that is *not* a closing tag, and finally the `</script>`.\n",
    "* **`HeadBlock`**: Does the same for `<head>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "const blockTokens = `\n",
    "    // Matches <script... > ... content ... </script>\n",
    "    ScriptBlock { \"<script\" ![>]* \">\" !(<)* \"</script>\" }\n",
    "\n",
    "    // Matches <head> ... content ... </head>\n",
    "    HeadBlock { \"<head>\" !(<)* \"</head>\" }\n",
    "`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741751fc",
   "metadata": {},
   "source": [
    "### 5. Content and Fallback\n",
    "\n",
    "Finally, we define what counts as actual text content.\n",
    "\n",
    "* **`Content`**: Any sequence of characters that is *not* a start of a tag (`<`), an entity start (`&`), or a newline.\n",
    "* **`any`**: A fallback for single characters that don't match anything else (safety net).\n",
    "\n",
    "We then close the `@tokens` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "const contentTokens = `\n",
    "    Content { ![<&\\\\n\\\\r]+ }\n",
    "    any { _ }\n",
    "  }\n",
    "`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6e522",
   "metadata": {},
   "source": [
    "### 6. Building the Parser\n",
    "\n",
    "Now we concatenate all parts to form the complete grammar string and compile it using `buildParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bf69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "const finalGrammar = \n",
    "    entryPoint + \n",
    "    basicTokens + \n",
    "    entityTokens + \n",
    "    blockTokens + \n",
    "    contentTokens;\n",
    "\n",
    "const parser = buildParser(finalGrammar);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba083c1",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "To convert the entities back to readable text, we use these helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c24881a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "function decodeEntity(entity: string): string {\n",
    "    const raw = entity.substring(1, entity.length - 1);\n",
    "    const map: Record<string, string> = {\n",
    "        'amp': '&', 'lt': '<', 'gt': '>', 'quot': '\"', 'apos': \"'\",\n",
    "        'uuml': '√º', 'auml': '√§', 'ouml': '√∂', 'szlig': '√ü'\n",
    "    };\n",
    "    return map[raw] || entity;\n",
    "}\n",
    "\n",
    "function decodeUnicode(unicode: string): string {\n",
    "    const code = parseInt(unicode);\n",
    "    return String.fromCodePoint(code);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16b4855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√§\n"
     ]
    }
   ],
   "source": [
    "decodeHTML(\"&auml;\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ec39e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&euml;\n"
     ]
    }
   ],
   "source": [
    "decodeEntity(\"&euml;\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c56ace0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Ö´\n"
     ]
    }
   ],
   "source": [
    "decodeUnicode(\"8555\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb024ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê¢\n"
     ]
    }
   ],
   "source": [
    "decodeUnicode(\"128034\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d03233d",
   "metadata": {},
   "source": [
    "### The Definition of the Token `ANY` \n",
    "\n",
    "The `ANY` token is our \"catch-all\" for regular text content. It matches any sequence of characters that don't start an HTML tag or entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57375f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "const ANY : TokenType = createToken({\n",
    "  name: \"ANY\",\n",
    "  pattern: /[^<&\\r\\n]+/\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b8c48b",
   "metadata": {},
   "source": [
    "The pattern `/[^<&\\r\\n]+/` matches one or more characters that are not:\n",
    "\n",
    "- `<` (which would start an HTML tag)\n",
    "- `&` (which would start an HTML entity)\n",
    "- `\\r` or `\\n` (which are handled by LINEBREAK)\n",
    "\n",
    "**Important**: This token must be defined last among the `initial_mode` tokens. Chevrotain tries to match tokens in the order they appear in the mode definition, so more specific patterns (like `TAG`, `NAMED_ENTITY`) must come before this general pattern. Otherwise, `ANY` would greedily consume characters that should be matched by other tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d090ad",
   "metadata": {},
   "source": [
    "### The Definition of the Token `HEAD_END` \n",
    "\n",
    "The `HEAD_END` token marks the end of the HTML header section and triggers a return to normal text extraction mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "const HEAD_END : TokenType = createToken({\n",
    "  name: \"HEAD_END\",\n",
    "  pattern: /<\\/head>/i,\n",
    "  pop_mode: true\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060466b3",
   "metadata": {},
   "source": [
    "The pattern /<\\/head>/i matches:\n",
    "\n",
    "- An opening angle bracket `<`\n",
    "- A forward slash `\\/` (escaped because `/` has special meaning in regex)\n",
    "- The word \"head\"\n",
    "- A closing angle bracket `>`\n",
    "- The `i` flag makes it case-insensitive\n",
    "\n",
    "The `pop_mode: true` property tells Chevrotain to return to the previous mode (which was `initial_mode` before we pushed to `header_mode`). This token is only active in `header_mode`, not in the `initial mode` - that's why it will only match the closing tag, not cause conflicts with other patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de68e8c",
   "metadata": {},
   "source": [
    "### The Definition of the Token `SCRIPT_END`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c077291a",
   "metadata": {},
   "source": [
    "Similar to `HEAD_END`, the `SCRIPT_END` token marks the end of embedded JavaScript code and returns the lexer to normal mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "const SCRIPT_END : TokenType = createToken({\n",
    "  name: \"SCRIPT_END\",\n",
    "  pattern: /<\\/script>/i,\n",
    "  pop_mode: true\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47051f8",
   "metadata": {},
   "source": [
    "The pattern `/<\\/script>/i` matches the closing script tag with case-insensitive matching. Like `HEAD_END`, the `pop_mode: true` property returns the lexer to `initial_mode` after this token is matched.\n",
    "\n",
    "This token is only active in `script_mode`, ensuring that JavaScript code between `<script>` and `</script>` tags is completely ignored and not extracted as text content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348f07a",
   "metadata": {},
   "source": [
    "### The Definition of Content Tokens for Special Modes\n",
    "\n",
    "When the lexer is in `header_mode` or `script_mode`, we need tokens that will consume (and discard) all content until the respective end tag is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "const HeaderContent : TokenType = createToken({\n",
    "  name: \"HeaderContent\",\n",
    "  pattern: /(.|\\n)+?(?=<\\/head>)/i,\n",
    "  line_breaks: true,\n",
    "  group: Lexer.SKIPPED\n",
    "});\n",
    "\n",
    "const ScriptContent : TokenType = createToken({\n",
    "  name: \"ScriptContent\",\n",
    "  pattern: /(.|\\n)+?(?=<\\/script>)/i,\n",
    "  line_breaks: true,\n",
    "  group: Lexer.SKIPPED\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b6e7e",
   "metadata": {},
   "source": [
    "These patterns use advanced regex features:\n",
    "\n",
    "- `(.|\\n)+?` matches any character (`.`) or newline (`\\n`), one or more times, non-greedy (`+?`)\n",
    "- `(?=<\\/head>)` is a positive lookahead‚Äîit checks that the closing tag follows, but doesn't consume it\n",
    "- `line_breaks: true` is essential because these patterns span multiple lines\n",
    "- `group: Lexer.SKIPPED` ensures this content is discarded, not extracted\n",
    "\n",
    "The non-greedy match (`+?`) combined with the lookahead ensures that these tokens stop just before the end tag, allowing `HEAD_END` or `SCRIPT_END` to match correctly. Without the lookahead, the pattern might consume the end tag itself, preventing the mode switch back to `initial_mode`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8220d1",
   "metadata": {},
   "source": [
    "## Running the Scanner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a8f97",
   "metadata": {},
   "source": [
    "### Creating the Lexer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d36257",
   "metadata": {},
   "source": [
    "Now that all tokens are defined, we can create the actual Chevrotain lexer. The lexer is configured with multiple modes, each containing a specific set of active tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a599f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "const HtmlLexer : Lexer = new Lexer({\n",
    "  defaultMode: \"initial_mode\",\n",
    "  modes: {\n",
    "    initial_mode: [\n",
    "      HEAD_START,\n",
    "      SCRIPT_START,\n",
    "      LINEBREAK,\n",
    "      TAG,\n",
    "      NAMED_ENTITY,\n",
    "      UNICODE,\n",
    "      ANY\n",
    "    ],\n",
    "    header_mode: [\n",
    "      HEAD_END,\n",
    "      HeaderContent\n",
    "    ],\n",
    "    script_mode: [\n",
    "      SCRIPT_END,\n",
    "      ScriptContent\n",
    "    ]\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8bd31",
   "metadata": {},
   "source": [
    "The lexer configuration specifies:\n",
    "\n",
    "- `defaultMode`: The mode the lexer starts in (`initial_mode`)\n",
    "- `modes`: An object defining which tokens are active in each mode\n",
    "\n",
    "**Token order matters!** Within each mode, tokens are tried in the order they appear. Specific patterns (like `NAMED_ENTITY`, `UNICODE`) must come before general ones (like `ANY`) to ensure correct matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a70f7",
   "metadata": {},
   "source": [
    "### Processing Tokens\n",
    "\n",
    "After tokenization, we need to process the tokens and reconstruct the plain text. The `processTokens` function iterates through all recognized tokens and builds the output string.\n",
    "\n",
    "Instead of comparing token names as strings (which is prone to typos), we use Chevrotain's `tokenMatcher` utility. This ensures type safety and robustness, even if we rename our tokens later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e3a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "function processTokens(tokens: IToken[]): string {\n",
    "  let result : string = \"\";\n",
    "  \n",
    "  for (const token of tokens) {\n",
    "    if (tokenMatcher(token, LINEBREAK)) {\n",
    "      result += \"\\n\";\n",
    "    } \n",
    "    else if (tokenMatcher(token, NAMED_ENTITY)) {\n",
    "      const entityText: string = token.image;\n",
    "      const cleanEntity: string = entityText.replace(/^&|;$/g, \"\");\n",
    "      result += decodeHTML(`&${cleanEntity};`);\n",
    "    } \n",
    "    else if (tokenMatcher(token, UNICODE)) {\n",
    "      const unicodeText: string = token.image;\n",
    "      const cleanNumber: string = unicodeText.replace(/^&#|;$/g, \"\");\n",
    "      result += String.fromCodePoint(parseInt(cleanNumber, 10));\n",
    "    } \n",
    "    else if (tokenMatcher(token, ANY)) {\n",
    "      result += token.image;\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return result;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e5a15",
   "metadata": {},
   "source": [
    "Each token type is handled differently:\n",
    "\n",
    "- **`LINEBREAK`**: Outputs a single newline character. Since our lexer pattern already consumed sequences of whitespace and newlines, this effectively condenses them into one.\n",
    "- **`NAMED_ENTITY`**: Extracts the entity name (removing the leading `&` and optional trailing `;`) and converts it to a character using `decodeHTML`.\n",
    "- **`UNICODE`**: Extracts the numeric code (removing `&#` and optional `;`) and converts it to a character using `String.fromCodePoint`.\n",
    "- **`ANY`**: Outputs the matched text exactly as it appeared in the source.\n",
    "\n",
    "Note that tokens like `HEAD_START`, `SCRIPT_START`, `HEAD_END`, and `SCRIPT_END` are not handled here because they serve only as control signals for mode switching and do not produce content. Similarly, the `TAG` token is missing because it was marked as `SKIPPED` in the lexer definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b9d7b",
   "metadata": {},
   "source": [
    "### Tokenizing and Extracting Text\n",
    "\n",
    "Finally, we feed our HTML data into the lexer and extract the plain text. The tokenize method returns a lexingResult object containing:\n",
    "\n",
    "- `tokens`: An array of successfully recognized tokens\n",
    "- `errors`: An array of any lexing errors encountered\n",
    "\n",
    "Error checking is included for robustness, though with our `ANY` token as a catch-all, lexing errors should never occur. The extracted text is then printed to the console, showing the HTML document stripped of all tags and with entities properly converted to Unicode characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3fb84c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "const lexingResult: ILexingResult = HtmlLexer.tokenize(data);\n",
    "\n",
    "if (lexingResult.errors.length > 0) {\n",
    "  console.error(\"Lexing errors detected:\");\n",
    "\n",
    "  for (const error of lexingResult.errors) {\n",
    "    const charFromMessage: string | undefined = error.message.match(/->(.)<-/)?.[1];\n",
    "    const illegalChar: string = charFromMessage || data.substr(error.offset, error.length) || \"?\";\n",
    "\n",
    "    console.error(`  - Illegal character '${illegalChar}' at line ${error.line}.`);\n",
    "    console.error(`    This is the ${error.offset}th character.`);\n",
    "  }\n",
    "}\n",
    "\n",
    "const extractedText = processTokens(lexingResult.tokens as IToken[]);\n",
    "console.log(extractedText);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad975ec2",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "The result is clean, readable text extracted from the HTML source. All tags have been removed, HTML entities like `&uuml;` have been converted to their Unicode equivalents (√º), and numeric entities like `&#8555;` have been converted to their characters (‚Ö´)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010dde3",
   "metadata": {},
   "source": [
    "### Inspecting Individual Tokens\n",
    "\n",
    "For debugging or educational purposes, you can inspect each token individually to see how the lexer processed the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e0f32c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (const tok of lexingResult.tokens as IToken[]) {\n",
    "  console.log({\n",
    "    name: tok.tokenType.name,\n",
    "    image: tok.image,\n",
    "    startLine: tok.startLine,\n",
    "    startColumn: tok.startColumn\n",
    "  });\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c32cb",
   "metadata": {},
   "source": [
    "Each token object contains:\n",
    "\n",
    "- `tokenType.name`: The type of token (e.g., \"`LINEBREAK`\", \"`ANY`\")\n",
    "\n",
    "- `image`: The actual matched text from the source\n",
    "\n",
    "- `startLine` and `startColumn`: Position information for debugging\n",
    "\n",
    "This allows you to see exactly how Chevrotain broke down the HTML into individual tokens before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63b09b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
